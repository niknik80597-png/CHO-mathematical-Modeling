
"""
ML —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
–û–±—É—á–µ–Ω–∏–µ Random Forest –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏
"""

import numpy as np
import pandas as pd
import json
import os
import glob
from datetime import datetime
import pickle
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)


def load_training_data():
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–∏
    –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—à–∏–±–∫–∏ –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –∫–∞–∫ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    """
    print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML...")

    all_features = []
    all_targets = []
    batch_ids = []

    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏
    result_files = glob.glob("data/processed/simulation_*_comparison.csv")

    if not result_files:
        print("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è!")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞ –º–µ—Ö–∞–Ω–∏—Å—Ç–∏—á–µ—Å–∫—É—é –º–æ–¥–µ–ª—å")
        return None, None, None

    print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(result_files)} —Ñ–∞–π–ª–æ–≤ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏")

    for file_path in result_files:
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º batch_id –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            batch_id = os.path.basename(file_path).replace('simulation_', '').replace('_comparison.csv', '')
            print(f"  üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {batch_id}...")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            df = pd.read_csv(file_path)

            # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
            required_cols = ['time_h', 'model_TCD', 'model_G', 'model_Lac',
                             'model_NH4_gL', 'model_P', 'exp_titer_g_L']

            df_clean = df.dropna(subset=required_cols).copy()

            if len(df_clean) < 5:  # –ú–∏–Ω–∏–º—É–º 5 —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö
                print(f"  ‚ö†Ô∏è  {batch_id}: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(df_clean)} —Ç–æ—á–µ–∫)")
                continue

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (–æ—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–∏—Ç—Ä–∞)
            df_clean['titer_error'] = df_clean['exp_titer_g_L'] - df_clean['model_P']

            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ (features)
            features = pd.DataFrame()

            # 1. –û—Å–Ω–æ–≤–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏
            features['time_h'] = df_clean['time_h']
            features['TCD'] = df_clean['model_TCD']
            features['glucose'] = df_clean['model_G']
            features['lactate'] = df_clean['model_Lac']
            features['ammonium'] = df_clean['model_NH4_gL']
            features['model_titer'] = df_clean['model_P']
            features['exp_titer'] = df_clean['exp_titer_g_L']

            # 2. –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features['time_norm'] = features['time_h'] / features['time_h'].max()
            features['glucose_lactate_ratio'] = features['glucose'] / (features['lactate'] + 0.1)
            features['metabolic_quotient'] = features['lactate'] / (features['glucose'] + 0.1)

            # 3. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ (—Ç—Ä–µ–Ω–¥—ã)
            if len(features) > 1:
                features['titer_growth_rate'] = features['model_titer'].diff().fillna(0)
                features['glucose_change_rate'] = features['glucose'].diff().fillna(0)
                features['lactate_change_rate'] = features['lactate'].diff().fillna(0)

            # 4. –ö–≤–∞–¥—Ä–∞—Ç—ã –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
            features['TCD_squared'] = features['TCD'] ** 2
            features['glucose_squared'] = features['glucose'] ** 2
            features['TCD_times_time'] = features['TCD'] * features['time_norm']

            # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            target = df_clean['titer_error'].values

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º batch_id –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
            batch_array = np.array([batch_id] * len(features))

            # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ–±—â–∏–º –¥–∞–Ω–Ω—ã–º
            all_features.append(features)
            all_targets.append(target)
            batch_ids.append(batch_array)

            print(f"  ‚úÖ {batch_id}: {len(features)} —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö")

        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path}: {str(e)}")
            continue

    if not all_features:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return None, None, None

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
    X = pd.concat(all_features, ignore_index=True)
    y = np.concatenate(all_targets)
    batch_ids_array = np.concatenate(batch_ids)

    print(f"\nüìä –ó–ê–ì–†–£–ñ–ï–ù–û –î–ê–ù–ù–´–•:")
    print(f"  –í—Å–µ–≥–æ —Ç–æ—á–µ–∫: {len(X)}")
    print(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[1]}")
    print(f"  –î–∏–∞–ø–∞–∑–æ–Ω –æ—à–∏–±–æ–∫ —Ç–∏—Ç—Ä–∞: [{y.min():.3f}, {y.max():.3f}]")
    print(f"  –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {y.mean():.3f}")
    print(f"  –ü–∞—Ä—Ç–∏–∏: {np.unique(batch_ids_array)}")

    return X, y, batch_ids_array


def prepare_features(X):
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    """
    print("\nüîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_names = X.columns.tolist()

    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    X_processed = X.copy()

    # –ó–∞–º–µ–Ω—è–µ–º –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏ –Ω–∞ NaN
    X_processed = X_processed.replace([np.inf, -np.inf], np.nan)

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    for col in X_processed.columns:
        if X_processed[col].isnull().any():
            # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∑–∞–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∏–∞–Ω–æ–π
            if X_processed[col].dtype in ['float64', 'int64']:
                X_processed[col].fillna(X_processed[col].median(), inplace=True)

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_processed)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏
    os.makedirs("ml_models", exist_ok=True)
    with open("ml_models/scaler.pkl", "wb") as f:
        pickle.dump(scaler, f)

    print(f"‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
    print(f"  –ò—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {len(feature_names)}")
    print(f"  –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {X_scaled.shape[1]}")

    return X_scaled, feature_names, scaler


def train_random_forest(n_estimators=100, max_depth=10, test_size=0.2):
    """
    –û–±—É—á–µ–Ω–∏–µ Random Forest –º–æ–¥–µ–ª–∏
    """
    print("\nüå≤ –û–ë–£–ß–ï–ù–ò–ï RANDOM FOREST –ú–û–î–ï–õ–ò")
    print("=" * 50)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X_raw, y, batch_ids = load_training_data()
    if X_raw is None:
        return None, {}

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X, feature_names, scaler = prepare_features(X_raw)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(
        X, y, range(len(y)), test_size=test_size, random_state=42
    )

    print(f"\nüìä –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–•:")
    print(f"  –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} —Ç–æ—á–µ–∫")
    print(f"  –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} —Ç–æ—á–µ–∫")
    print(f"  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X_train.shape[1]}")

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("\nüîÑ –û–±—É—á–µ–Ω–∏–µ Random Forest...")

    model = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —è–¥—Ä–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        verbose=1
    )

    model.fit(X_train, y_train)

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    train_mae = mean_absolute_error(y_train, y_train_pred)
    test_mae = mean_absolute_error(y_test, y_test_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

    # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
    cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2', n_jobs=-1)

    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = dict(zip(feature_names, model.feature_importances_))
    sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)

    print(f"\n‚úÖ –ú–û–î–ï–õ–¨ –û–ë–£–ß–ï–ù–ê!")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    os.makedirs("ml_models", exist_ok=True)
    model_path = "ml_models/random_forest_model.pkl"

    with open(model_path, "wb") as f:
        pickle.dump(model, f)

    print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    plot_feature_importance(sorted_importance[:15])

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    plot_predictions(y_test, y_test_pred, batch_ids[idx_test])

    # –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫
    metrics = {
        'train_r2': train_r2,
        'test_r2': test_r2,
        'train_mae': train_mae,
        'test_mae': test_mae,
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'feature_importance': feature_importance,
        'model_params': {
            'n_estimators': n_estimators,
            'max_depth': max_depth,
            'test_size': test_size
        },
        'training_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'n_samples': len(X),
        'n_features': X.shape[1]
    }

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    with open("ml_models/training_metrics.json", "w") as f:
        json.dump(metrics, f, indent=2)

    return model, metrics


def plot_feature_importance(importance_list):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    features = [item[0] for item in importance_list]
    importance = [item[1] for item in importance_list]

    plt.figure(figsize=(10, 6))
    bars = plt.barh(range(len(features)), importance, color='steelblue')
    plt.yticks(range(len(features)), features)
    plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞')
    plt.title('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ Random Forest')
    plt.gca().invert_yaxis()

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –±–∞—Ä—ã
    for i, (bar, val) in enumerate(zip(bars, importance)):
        plt.text(val + 0.001, bar.get_y() + bar.get_height() / 2,
                 f'{val:.3f}', va='center', fontsize=9)

    plt.tight_layout()
    os.makedirs("ml_models/plots", exist_ok=True)
    plt.savefig("ml_models/plots/feature_importance.png", dpi=300, bbox_inches='tight')
    plt.show()


def plot_predictions(y_true, y_pred, batch_ids):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    plt.figure(figsize=(12, 5))

    # –ì—Ä–∞—Ñ–∏–∫ 1: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    plt.subplot(1, 2, 1)
    plt.scatter(y_true, y_pred, alpha=0.6, c='steelblue', edgecolor='k', linewidth=0.5)

    # –õ–∏–Ω–∏—è –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    min_val = min(y_true.min(), y_pred.min())
    max_val = max(y_true.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='–ò–¥–µ–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ')

    plt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞')
    plt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞')
    plt.title('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Random Forest')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # –ì—Ä–∞—Ñ–∏–∫ 2: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
    plt.subplot(1, 2, 2)
    errors = y_pred - y_true
    plt.hist(errors, bins=30, color='steelblue', edgecolor='black', alpha=0.7)
    plt.xlabel('–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫\n–°—Ä–µ–¥–Ω–µ–µ: {errors.mean():.4f}, STD: {errors.std():.4f}')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig("ml_models/plots/predictions.png", dpi=300, bbox_inches='tight')
    plt.show()


def evaluate_ml_model(model=None):
    """
    –û—Ü–µ–Ω–∫–∞ ML –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    print("\nüìà –û–¶–ï–ù–ö–ê ML –ú–û–î–ï–õ–ò")

    if model is None:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        model_path = "ml_models/random_forest_model.pkl"
        if not os.path.exists(model_path):
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return None

        with open(model_path, "rb") as f:
            model = pickle.load(f)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X_raw, y, batch_ids = load_training_data()
    if X_raw is None:
        return None

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∑–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π scaler)
    scaler_path = "ml_models/scaler.pkl"
    if os.path.exists(scaler_path):
        with open(scaler_path, "rb") as f:
            scaler = pickle.load(f)
        X = scaler.transform(X_raw)
    else:
        X, _, _ = prepare_features(X_raw)

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    y_pred = model.predict(X)

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    r2 = r2_score(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    rmse = np.sqrt(mean_squared_error(y, y_pred))

    # –û—Ü–µ–Ω–∫–∞ –ø–æ –ø–∞—Ä—Ç–∏—è–º
    batch_metrics = {}
    unique_batches = np.unique(batch_ids)

    for batch in unique_batches:
        mask = batch_ids == batch
        y_batch = y[mask]
        y_pred_batch = y_pred[mask]

        if len(y_batch) > 1:  # –ù—É–∂–Ω–æ —Ö–æ—Ç—è –±—ã 2 —Ç–æ—á–∫–∏ –¥–ª—è R¬≤
            batch_r2 = r2_score(y_batch, y_pred_batch)
            batch_mae = mean_absolute_error(y_batch, y_pred_batch)
            batch_metrics[batch] = {
                'r2': batch_r2,
                'mae': batch_mae,
                'n_points': len(y_batch)
            }

    results = {
        'overall_r2': r2,
        'overall_mae': mae,
        'overall_rmse': rmse,
        'batch_metrics': batch_metrics,
        'mean_error': np.mean(np.abs(y_pred - y)),
        'std_error': np.std(y_pred - y),
        'n_samples': len(y)
    }

    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò:")
    print(f"  –û–±—â–∏–π R¬≤: {r2:.3f}")
    print(f"  –û–±—â–∞—è MAE: {mae:.3f}")
    print(f"  –û–±—â–∞—è RMSE: {rmse:.3f}")
    print(f"  –°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {results['mean_error']:.4f}")

    print(f"\nüìà –ü–û –ü–ê–†–¢–ò–Ø–ú:")
    for batch, metrics in batch_metrics.items():
        print(f"  {batch}: R¬≤={metrics['r2']:.3f}, MAE={metrics['mae']:.3f} ({metrics['n_points']} —Ç–æ—á–µ–∫)")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –ø–∞—Ä—Ç–∏—è–º
    plot_batch_results(results, batch_ids, y, y_pred)

    return results


def plot_batch_results(results, batch_ids, y_true, y_pred):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –ø–∞—Ä—Ç–∏—è–º"""
    unique_batches = np.unique(batch_ids)
    n_batches = len(unique_batches)

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # –ì—Ä–∞—Ñ–∏–∫ 1: R¬≤ –ø–æ –ø–∞—Ä—Ç–∏—è–º
    ax = axes[0, 0]
    batch_r2 = [results['batch_metrics'].get(batch, {}).get('r2', 0) for batch in unique_batches]
    bars = ax.bar(range(n_batches), batch_r2, color='steelblue')
    ax.set_xticks(range(n_batches))
    ax.set_xticklabels(unique_batches, rotation=45)
    ax.set_ylabel('R¬≤')
    ax.set_title('–ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –ø–∞—Ä—Ç–∏—è–º (R¬≤)')
    ax.grid(True, alpha=0.3)

    # –ì—Ä–∞—Ñ–∏–∫ 2: –û—à–∏–±–∫–∏ –ø–æ –ø–∞—Ä—Ç–∏—è–º
    ax = axes[0, 1]
    batch_errors = []
    for batch in unique_batches:
        mask = batch_ids == batch
        if np.any(mask):
            error = np.mean(np.abs(y_pred[mask] - y_true[mask]))
            batch_errors.append(error)

    bars = ax.bar(range(n_batches), batch_errors, color='coral')
    ax.set_xticks(range(n_batches))
    ax.set_xticklabels(unique_batches, rotation=45)
    ax.set_ylabel('–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')
    ax.set_title('–û—à–∏–±–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –ø–∞—Ä—Ç–∏—è–º')
    ax.grid(True, alpha=0.3)

    # –ì—Ä–∞—Ñ–∏–∫ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫
    ax = axes[1, 0]
    errors = y_pred - y_true
    ax.hist(errors, bins=30, color='lightgreen', edgecolor='black', alpha=0.7)
    ax.axvline(x=0, color='red', linestyle='--', alpha=0.7)
    ax.set_xlabel('–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è')
    ax.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    ax.set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫\n–°—Ä–µ–¥–Ω–µ–µ: {errors.mean():.4f}, STD: {errors.std():.4f}')
    ax.grid(True, alpha=0.3)

    # –ì—Ä–∞—Ñ–∏–∫ 4: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    ax = axes[1, 1]
    scatter = ax.scatter(y_true, y_pred, c=range(len(y_true)),
                         cmap='viridis', alpha=0.6, edgecolor='k', linewidth=0.5)

    min_val = min(y_true.min(), y_pred.min())
    max_val = max(y_true.max(), y_pred.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7, label='–ò–¥–µ–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ')

    ax.set_xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞')
    ax.set_ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞')
    ax.set_title('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è')
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.colorbar(scatter, ax=ax, label='–ü–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä —Ç–æ—á–∫–∏')

    plt.suptitle('–û—Ü–µ–Ω–∫–∞ Random Forest –º–æ–¥–µ–ª–∏ –ø–æ –ø–∞—Ä—Ç–∏—è–º', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig("ml_models/plots/batch_evaluation.png", dpi=300, bbox_inches='tight')
    plt.show()


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print("ü§ñ ML –¢—Ä–µ–Ω–µ—Ä –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ CHO")
    model, metrics = train_random_forest()

    if model:
        print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìä –¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (R¬≤): {metrics['test_r2']:.3f}")
    else:
        print("‚ùå –û–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å")
